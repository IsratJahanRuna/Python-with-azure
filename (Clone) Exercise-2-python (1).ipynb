{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cdacc59-00e3-4a74-9b30-3341b0cffa0b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# COMP.CS.320 Data-Intensive Programming, Exercise 2\n",
    "\n",
    "This exercise contains basic tasks of data processing using Spark and DataFrames. The tasks can be done in either Scala or Python. This is the **Python** version, switch to the Scala version if you want to do the tasks in Scala.\n",
    "\n",
    "Each task has its own cell for the code. Add your solutions to the cells. You are free to add more cells if you feel it is necessary. There are cells with example outputs or test code following most of the tasks.\n",
    "\n",
    "Don't forget to submit your solutions to Moodle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15ff864b-6603-4065-a797-efce8813a983",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# some imports that might be required in the tasks\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from pyspark.sql import functions\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import Row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93883aa9-ea4b-4a1e-bd2d-b680cafe9576",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Task 1 - Create DataFrame\n",
    "\n",
    "As mentioned in the tutorial notebook, Azure Storage Account and Azure Data Lake Storage Gen2 are used in the course to provide a place to read and write data files.\n",
    "In the [Shared container](https://portal.azure.com/#view/Microsoft_Azure_Storage/ContainerMenuBlade/~/overview/storageAccountId/%2Fsubscriptions%2Fe0c78478-e7f8-429c-a25f-015eae9f54bb%2FresourceGroups%2Ftuni-cs320-f2023-rg%2Fproviders%2FMicrosoft.Storage%2FstorageAccounts%2Ftunics320f2023gen2/path/shared/etag/%220x8DBB0695B02FFFE%22/defaultEncryptionScope/%24account-encryption-key/denyEncryptionScopeOverride~/false/defaultId//publicAccessVal/None) in the `exercises/ex2` folder is file `rdu-weather-history.csv` that contains weather data in CSV format.\n",
    "The direct address for the data file is: `abfss://shared@tunics320f2023gen2.dfs.core.windows.net/exercises/ex2/rdu-weather-history.csv`\n",
    "\n",
    "Read the data from the CSV file into DataFrame called weatherDataFrame. Let Spark infer the schema for the data.\n",
    "\n",
    "Print out the schema.\n",
    "Study the schema and compare it to the data in the CSV file. Do they match?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51cfc3a2-5eb8-4339-8c71-74c7d49bc88b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- date: date (nullable = true)\n |-- temperaturemin: double (nullable = true)\n |-- temperaturemax: double (nullable = true)\n |-- precipitation: double (nullable = true)\n |-- snowfall: double (nullable = true)\n |-- snowdepth: double (nullable = true)\n |-- avgwindspeed: double (nullable = true)\n |-- fastest2minwinddir: integer (nullable = true)\n |-- fastest2minwindspeed: double (nullable = true)\n |-- fastest5secwinddir: integer (nullable = true)\n |-- fastest5secNo: double (nullable = true)\n |-- windspeed: string (nullable = true)\n |-- fog: string (nullable = true)\n |-- fogheavy: string (nullable = true)\n |-- mist: string (nullable = true)\n |-- rain: string (nullable = true)\n |-- fogground: string (nullable = true)\n |-- ice: string (nullable = true)\n |-- glaze: string (nullable = true)\n |-- drizzle: string (nullable = true)\n |-- snow: string (nullable = true)\n |-- freezingrain: string (nullable = true)\n |-- smokehaze: string (nullable = true)\n |-- thunder: string (nullable = true)\n |-- highwind: string (nullable = true)\n |-- hail: string (nullable = true)\n |-- blowingsnow: string (nullable = true)\n |-- dust: string (nullable = true)\n\n+----------+--------------+--------------+-------------+--------+---------+------------+------------------+--------------------+------------------+-------------+---------+---+--------+----+----+---------+---+-----+-------+----+------------+---------+-------+--------+----+-----------+----+\n|      date|temperaturemin|temperaturemax|precipitation|snowfall|snowdepth|avgwindspeed|fastest2minwinddir|fastest2minwindspeed|fastest5secwinddir|fastest5secNo|windspeed|fog|fogheavy|mist|rain|fogground|ice|glaze|drizzle|snow|freezingrain|smokehaze|thunder|highwind|hail|blowingsnow|dust|\n+----------+--------------+--------------+-------------+--------+---------+------------+------------------+--------------------+------------------+-------------+---------+---+--------+----+----+---------+---+-----+-------+----+------------+---------+-------+--------+----+-----------+----+\n|2008-05-20|          57.9|          82.9|         0.43|     0.0|      0.0|       10.51|               230|               25.05|               220|        31.99|      Yes| No|     Yes| Yes|  No|       No| No|   No|     No|  No|          No|      Yes|     No|     Yes|  No|         No|  No|\n|2008-05-22|          48.0|          78.1|          0.0|     0.0|      0.0|        4.03|               230|               16.11|               280|        21.03|      Yes| No|      No|  No|  No|       No| No|   No|     No|  No|          No|       No|     No|      No|  No|         No|  No|\n|2008-05-23|          52.0|          79.0|          0.0|     0.0|      0.0|         4.7|                70|               10.07|               100|        14.99|       No| No|      No|  No|  No|       No| No|   No|     No|  No|          No|       No|     No|      No|  No|         No|  No|\n|2008-06-07|          73.9|         100.0|          0.0|     0.0|      0.0|        5.59|               230|               16.11|               220|        21.92|       No| No|      No|  No|  No|       No| No|   No|     No|  No|          No|       No|     No|      No|  No|         No|  No|\n|2008-06-22|          64.9|          87.1|         0.93|     0.0|      0.0|        6.93|               200|               23.04|               200|        29.97|      Yes| No|      No| Yes|  No|       No| No|   No|     No|  No|         Yes|      Yes|     No|     Yes|  No|        Yes|  No|\n|2008-06-27|          71.1|          96.1|          0.0|     0.0|      0.0|        7.83|               220|                17.9|               220|        21.92|      Yes| No|      No| Yes|  No|       No| No|   No|     No|  No|         Yes|      Yes|     No|     Yes|  No|        Yes|  No|\n|2008-06-29|          69.1|          95.0|         0.82|     0.0|      0.0|       11.41|               230|               25.05|               230|        29.08|      Yes| No|      No| Yes|  No|       No| No|   No|     No|  No|          No|      Yes|     No|     Yes|  No|         No|  No|\n|2008-07-07|          69.1|          87.1|          0.0|     0.0|      0.0|         9.4|               230|                17.0|               220|        21.03|      Yes| No|     Yes| Yes|  No|       No| No|   No|     No|  No|          No|       No|     No|      No|  No|         No|  No|\n|2008-07-09|          70.0|          90.0|         0.97|     0.0|      0.0|       10.51|               300|               23.04|               300|        36.01|      Yes| No|     Yes| Yes|  No|       No| No|   No|     No|  No|          No|      Yes|     No|     Yes|  No|         No|  No|\n|2008-07-17|          64.0|          89.1|          0.0|     0.0|      0.0|        5.37|                50|               14.99|               100|        19.91|      Yes| No|     Yes|  No|  No|       No| No|   No|     No|  No|         Yes|       No|     No|      No|  No|         No|  No|\n|2008-07-19|          70.0|          90.0|         0.76|     0.0|      0.0|        3.36|               150|               19.91|               130|        23.94|      Yes| No|     Yes| Yes|  No|       No| No|   No|     No|  No|          No|       No|     No|     Yes|  No|         No|  No|\n|2008-09-04|          64.0|          90.0|          0.0|     0.0|      0.0|        4.47|               170|               10.07|               180|        14.09|      Yes| No|     Yes|  No|  No|       No| No|   No|     No|  No|         Yes|       No|     No|      No|  No|         No|  No|\n|2008-09-13|          72.0|          91.0|          0.0|     0.0|      0.0|        6.26|               240|               14.09|               230|         17.0|      Yes| No|     Yes|  No|  No|       No| No|   No|     No|  No|          No|       No|     No|      No|  No|         No|  No|\n|2008-09-17|          60.1|          73.9|          0.0|     0.0|      0.0|         3.8|                30|               10.07|                50|        14.09|      Yes| No|     Yes| Yes|  No|       No| No|   No|     No|  No|          No|       No|     No|      No|  No|         No|  No|\n|2008-09-24|          53.1|          70.0|          0.0|     0.0|      0.0|        12.3|                40|               23.04|                20|        29.97|       No| No|      No|  No|  No|       No| No|   No|     No|  No|          No|       No|     No|      No|  No|         No|  No|\n|2008-09-26|          63.0|          81.0|         0.96|     0.0|      0.0|       10.07|                30|               21.92|                30|        31.09|      Yes| No|     Yes| Yes|  No|       No| No|   No|     No|  No|          No|       No|     No|     Yes|  No|         No|  No|\n|2008-10-10|          64.9|          73.0|         0.02|     0.0|      0.0|        6.71|                50|               16.11|                40|         17.9|      Yes| No|     Yes| Yes|  No|       No| No|   No|     No|  No|          No|       No|     No|     Yes|  No|         No|  No|\n|2008-10-20|          35.1|          66.9|          0.0|     0.0|      0.0|        2.24|               220|               10.07|               220|        12.97|      Yes| No|      No|  No|  No|       No| No|   No|     No|  No|          No|       No|     No|      No|  No|         No|  No|\n|2008-10-26|          44.1|          70.0|          0.0|     0.0|      0.0|        3.13|               230|               12.08|              NULL|         NULL|      Yes| No|      No|  No|  No|       No| No|   No|     No|  No|          No|       No|     No|      No|  No|         No|  No|\n|2008-11-05|          52.0|          68.0|          0.0|     0.0|      0.0|        6.93|                40|               14.99|                30|        21.03|      Yes| No|     Yes| Yes|  No|       No| No|   No|     No|  No|          No|       No|     No|      No|  No|         No|  No|\n+----------+--------------+--------------+-------------+--------+---------+------------+------------------+--------------------+------------------+-------------+---------+---+--------+----+----+---------+---+-----+-------+----+------------+---------+-------+--------+----+-----------+----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "\n",
    "spark: SparkSession = SparkSession.builder.appName(\"WeatherData\").getOrCreate()\n",
    "\n",
    "filePath: str = \"abfss://shared@tunics320f2023gen2.dfs.core.windows.net/exercises/ex2/rdu-weather-history.csv\"\n",
    "\n",
    "weatherDataFrame: DataFrame = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(filePath)\n",
    "\n",
    "weatherDataFrame.printSchema()\n",
    "\n",
    "weatherDataFrame.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eebaf456-e405-4f26-b1ca-6514c71eaf6b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Example output for task 1 (only the first few lines):\n",
    "\n",
    "```text\n",
    "root\n",
    " |-- date: date (nullable = true)\n",
    " |-- temperaturemin: double (nullable = true)\n",
    " |-- temperaturemax: double (nullable = true)\n",
    " |-- precipitation: double (nullable = true)\n",
    " |-- snowfall: double (nullable = true)\n",
    " |-- snowdepth: double (nullable = true)\n",
    " |-- avgwindspeed: double (nullable = true)\n",
    " ...\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d105aac-e8c0-4cec-81e4-1f59fe41e8b9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Task 2 - The first items from DataFrame\n",
    "\n",
    "Fetch the first **five** rows of the weather dataframe and print their contents. You can use the DataFrame variable from task 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a0dc1e1-595e-4269-8da5-d4a53fa1f44a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[datetime.date(2008, 5, 20), 57.9, 82.9, 0.43, 0.0, 0.0, 10.51, 230, 25.05, 220, 31.99, 'Yes', 'No', 'Yes', 'Yes', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'Yes', 'No', 'Yes', 'No', 'No', 'No']\n[datetime.date(2008, 5, 22), 48.0, 78.1, 0.0, 0.0, 0.0, 4.03, 230, 16.11, 280, 21.03, 'Yes', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No']\n[datetime.date(2008, 5, 23), 52.0, 79.0, 0.0, 0.0, 0.0, 4.7, 70, 10.07, 100, 14.99, 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No']\n[datetime.date(2008, 6, 7), 73.9, 100.0, 0.0, 0.0, 0.0, 5.59, 230, 16.11, 220, 21.92, 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No']\n[datetime.date(2008, 6, 22), 64.9, 87.1, 0.93, 0.0, 0.0, 6.93, 200, 23.04, 200, 29.97, 'Yes', 'No', 'No', 'Yes', 'No', 'No', 'No', 'No', 'No', 'No', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No']\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from pyspark.sql import Row\n",
    "weatherSample: List[Row] = weatherDataFrame.head(5)\n",
    "\n",
    "print(*[list(row.asDict().values()) for row in weatherSample], sep=\"\\n\")  # prints each Row to its own line\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54bbfd2d-7cee-44a3-b018-99d62a4b1f70",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Example output for task 2:\n",
    "\n",
    "```text\n",
    "[datetime.date(2008, 5, 20), 57.9, 82.9, 0.43, 0.0, 0.0, 10.51, 230, 25.05, 220, 31.99, 'Yes', 'No', 'Yes', 'Yes', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'Yes', 'No', 'Yes', 'No', 'No', 'No']\n",
    "[datetime.date(2008, 5, 22), 48.0, 78.1, 0.0, 0.0, 0.0, 4.03, 230, 16.11, 280, 21.03, 'Yes', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No']\n",
    "[datetime.date(2008, 5, 23), 52.0, 79.0, 0.0, 0.0, 0.0, 4.7, 70, 10.07, 100, 14.99, 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No']\n",
    "[datetime.date(2008, 6, 7), 73.9, 100.0, 0.0, 0.0, 0.0, 5.59, 230, 16.11, 220, 21.92, 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No']\n",
    "[datetime.date(2008, 6, 22), 64.9, 87.1, 0.93, 0.0, 0.0, 6.93, 200, 23.04, 200, 29.97, 'Yes', 'No', 'No', 'Yes', 'No', 'No', 'No', 'No', 'No', 'No', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88c320da-bd69-4a79-80b6-442ce90d759c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Task 3 - Minimum and maximum\n",
    "\n",
    "Find the minimum temperature and the maximum temperature from the whole data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15d3d180-b14e-492d-b38f-62f80f047a44",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min temperature is 4.1\nMax temperature is 105.1\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"WeatherAnalysis\").getOrCreate()\n",
    "filePath = \"abfss://shared@tunics320f2023gen2.dfs.core.windows.net/exercises/ex2/rdu-weather-history.csv\"\n",
    "weatherDataFrame = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(filePath)\n",
    "\n",
    "minTemp: float =  weatherDataFrame.select(F.min(\"temperaturemin\")).collect()[0][0]\n",
    "maxTemp: float = weatherDataFrame.select(F.max(\"temperaturemax\")).collect()[0][0]\n",
    "\n",
    "print(f\"Min temperature is {minTemp}\")\n",
    "print(f\"Max temperature is {maxTemp}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecd683da-0c43-4321-b28d-6b7bb2703bf0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct result: minimum temperature is 4.1 °F (-15,5 °C)\ncorrect result: maximum temperature is 105.1 °F (40.6 °C)\n"
     ]
    }
   ],
   "source": [
    "if 4.05 < minTemp < 4.15:\n",
    "    print(\"correct result: minimum temperature is 4.1 °F (-15,5 °C)\")\n",
    "else:\n",
    "    print(f\"wrong result: {minTemp} != 4.1\")\n",
    "\n",
    "if 105.05 < maxTemp < 105.15:\n",
    "    print(\"correct result: maximum temperature is 105.1 °F (40.6 °C)\")\n",
    "else:\n",
    "    print(f\"wrong result: {maxTemp} != 105.1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7057d3cf-9819-4fdc-acac-bd42152fb682",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Task 4 - Adding a column\n",
    "\n",
    "Add a new column `year` to the weatherDataFrame and print out the schema for the new DataFrame.\n",
    "\n",
    "The type of the new column should be integer and value calculated from column `date`.\n",
    "You can use function `functions.year` from `pyspark.sql`\n",
    "\n",
    "See documentation: [https://spark.apache.org/docs/3.4.1/api/python/reference/pyspark.sql/api/pyspark.sql.functions.year.html#pyspark.sql.functions.year](https://spark.apache.org/docs/3.4.1/api/python/reference/pyspark.sql/api/pyspark.sql.functions.year.html#pyspark.sql.functions.year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ecbf8d5-573b-47f4-89dd-bd0af82fc195",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- date: date (nullable = true)\n |-- temperaturemin: double (nullable = true)\n |-- temperaturemax: double (nullable = true)\n |-- precipitation: double (nullable = true)\n |-- snowfall: double (nullable = true)\n |-- snowdepth: double (nullable = true)\n |-- avgwindspeed: double (nullable = true)\n |-- fastest2minwinddir: integer (nullable = true)\n |-- fastest2minwindspeed: double (nullable = true)\n |-- fastest5secwinddir: integer (nullable = true)\n |-- fastest5secNo: double (nullable = true)\n |-- windspeed: string (nullable = true)\n |-- fog: string (nullable = true)\n |-- fogheavy: string (nullable = true)\n |-- mist: string (nullable = true)\n |-- rain: string (nullable = true)\n |-- fogground: string (nullable = true)\n |-- ice: string (nullable = true)\n |-- glaze: string (nullable = true)\n |-- drizzle: string (nullable = true)\n |-- snow: string (nullable = true)\n |-- freezingrain: string (nullable = true)\n |-- smokehaze: string (nullable = true)\n |-- thunder: string (nullable = true)\n |-- highwind: string (nullable = true)\n |-- hail: string (nullable = true)\n |-- blowingsnow: string (nullable = true)\n |-- dust: string (nullable = true)\n |-- year: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date\n",
    "\n",
    "spark = SparkSession.builder.appName(\"WeatherAnalysis\").getOrCreate()\n",
    "filePath = \"abfss://shared@tunics320f2023gen2.dfs.core.windows.net/exercises/ex2/rdu-weather-history.csv\"\n",
    "weatherDataFrame = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(filePath)\n",
    "\n",
    "weatherDataFrame = weatherDataFrame.withColumn(\"date\", to_date(col(\"date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "weatherDataFrameWithYear = weatherDataFrame.withColumn(\"year\", F.year(col(\"date\")))\n",
    "\n",
    "weatherDataFrameWithYear.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45690e64-0fab-4bb6-b4b5-bb68d8f3274b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Example output for task 4 (only the last few lines):\n",
    "\n",
    "```text\n",
    "...\n",
    " |-- highwind: string (nullable = true)\n",
    " |-- hail: string (nullable = true)\n",
    " |-- blowingsnow: string (nullable = true)\n",
    " |-- dust: string (nullable = true)\n",
    " |-- year: integer (nullable = true)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a39d79f-e147-4a46-ba1a-d74450e2b596",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Task 5 - Aggregated DataFrame 1\n",
    "\n",
    "Find the minimum and the maximum temperature for each year.\n",
    "\n",
    "Sort the resulting DataFrame based on year so that the latest year is in the first row and the earliest year is in the last row.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed827f31-85a9-4746-9a92-b4ce4978d94b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------+---------------+\n|year|min_temperature|max_temperature|\n+----+---------------+---------------+\n|2007|           15.1|          105.1|\n|2018|            4.1|           98.1|\n|2015|            7.2|          100.0|\n|2013|           18.0|           96.1|\n|2014|            7.2|           98.1|\n|2012|           19.0|          105.1|\n|2009|           10.9|           99.0|\n|2016|           15.3|           99.0|\n|2010|           15.1|          102.0|\n|2011|           16.0|          104.0|\n|2008|           15.1|          100.9|\n|2017|            9.1|          102.0|\n+----+---------------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "aggregatedDF = weatherDataFrameWithYear.groupBy(\"year\") \\\n",
    "    .agg(F.min(\"temperaturemin\").alias(\"min_temperature\"), F.max(\"temperaturemax\").alias(\"max_temperature\"))\n",
    "\n",
    "aggregatedDF.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "730231a1-5ae3-4c1c-a29d-28c851d147e2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Example output for task 5:\n",
    "\n",
    "```text\n",
    "+----+---------------+---------------+\n",
    "|year|min_temperature|max_temperature|\n",
    "+----+---------------+---------------+\n",
    "|2018|            4.1|           98.1|\n",
    "|2017|            9.1|          102.0|\n",
    "|2016|           15.3|           99.0|\n",
    "|2015|            7.2|          100.0|\n",
    "|2014|            7.2|           98.1|\n",
    "|2013|           18.0|           96.1|\n",
    "|2012|           19.0|          105.1|\n",
    "|2011|           16.0|          104.0|\n",
    "|2010|           15.1|          102.0|\n",
    "|2009|           10.9|           99.0|\n",
    "|2008|           15.1|          100.9|\n",
    "|2007|           15.1|          105.1|\n",
    "+----+---------------+---------------+\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e7f5f02-4fd7-49d6-8af1-d6f192edde83",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Task 6 - Aggregated DataFrame 2\n",
    "\n",
    "Expanding from task 5, create a DataFrame that contains the following for each year:\n",
    "\n",
    "- the minimum temperature\n",
    "- the maximum temperature\n",
    "- the number of entries (as in rows in the original data) there are for that year\n",
    "- the average wind speed (rounded to 2 decimal precision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d24877d5-c077-4b86-a46a-f77d400bf690",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------+---------------+-----------------+------------------+\n|year|min_temperature|max_temperature|number_of_entries|average_wind_speed|\n+----+---------------+---------------+-----------------+------------------+\n|2007|           15.1|          105.1|              365|              6.14|\n|2018|            4.1|           98.1|              228|              6.55|\n|2015|            7.2|          100.0|              365|              5.44|\n|2013|           18.0|           96.1|              365|              5.51|\n|2014|            7.2|           98.1|              365|              5.56|\n|2012|           19.0|          105.1|              366|              5.41|\n|2009|           10.9|           99.0|              365|              6.13|\n|2016|           15.3|           99.0|              366|              5.78|\n|2010|           15.1|          102.0|              365|              5.49|\n|2011|           16.0|          104.0|              365|              5.84|\n|2008|           15.1|          100.9|              366|              6.49|\n|2017|            9.1|          102.0|              365|              6.25|\n+----+---------------+---------------+-----------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"WeatherAnalysis\").getOrCreate()\n",
    "\n",
    "file_path = \"abfss://shared@tunics320f2023gen2.dfs.core.windows.net/exercises/ex2/rdu-weather-history.csv\"\n",
    "\n",
    "weatherDataFrameWithYear = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(file_path)\n",
    "\n",
    "weatherDataFrameWithYear = weatherDataFrameWithYear.withColumn(\"year\", F.year(weatherDataFrameWithYear[\"date\"]))\n",
    "\n",
    "task6DF = (\n",
    "    weatherDataFrameWithYear\n",
    "    .groupBy(\"year\")\n",
    "    .agg(\n",
    "        F.min(\"temperaturemin\").alias(\"min_temperature\"),\n",
    "        F.max(\"temperaturemax\").alias(\"max_temperature\"),\n",
    "        F.count(\"*\").alias(\"number_of_entries\"),\n",
    "        F.round(F.avg(\"avgwindspeed\"), 2).alias(\"average_wind_speed\")\n",
    "    )\n",
    ")\n",
    "\n",
    "task6DF.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "264fee26-3bdc-4b85-a12d-d9edfb47d4d7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Example output for task 6:\n",
    "\n",
    "```text\n",
    "+----+---------------+---------------+-------+-------------+\n",
    "|year|min_temperature|max_temperature|entries|avg_windspeed|\n",
    "+----+---------------+---------------+-------+-------------+\n",
    "|2007|           15.1|          105.1|    365|         6.14|\n",
    "|2018|            4.1|           98.1|    228|         6.55|\n",
    "|2015|            7.2|          100.0|    365|         5.44|\n",
    "|2013|           18.0|           96.1|    365|         5.51|\n",
    "|2014|            7.2|           98.1|    365|         5.56|\n",
    "|2012|           19.0|          105.1|    366|         5.41|\n",
    "|2009|           10.9|           99.0|    365|         6.13|\n",
    "|2016|           15.3|           99.0|    366|         5.78|\n",
    "|2010|           15.1|          102.0|    365|         5.49|\n",
    "|2011|           16.0|          104.0|    365|         5.84|\n",
    "|2008|           15.1|          100.9|    366|         6.49|\n",
    "|2017|            9.1|          102.0|    365|         6.25|\n",
    "+----+---------------+---------------+-------+-------------+\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cd5a758-4d16-441b-aca3-9b170887a78a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Task 7 - Aggregated DataFrame 3\n",
    "\n",
    "Using the DataFrame created in task 6, `task6DF`, find the following values:\n",
    "\n",
    "- the minimum temperature for year 2012\n",
    "- the maximum temperature for year 2016\n",
    "- the number of entries for year 2018\n",
    "- the average wind speed for year 2008\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23ea3be2-d22f-4022-8063-8494f8213ba2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum temperature for year 2012: 19.0\nThe maximum temperature for year 2016: 99.0\nThe number of entries for year 2018: 228\nThe average wind speed for year 2008: 6.49\n"
     ]
    }
   ],
   "source": [
    "\n",
    "min2012: float = task6DF.filter(task6DF[\"year\"] == 2012).select(\"min_temperature\").first()[0]\n",
    "max2016: float = task6DF.filter(task6DF[\"year\"] == 2016).select(\"max_temperature\").first()[0]\n",
    "entries2018: int = task6DF.filter(task6DF[\"year\"] == 2018).select(\"number_of_entries\").first()[0]\n",
    "wind2008: float = task6DF.filter(task6DF[\"year\"] == 2008).select(\"average_wind_speed\").first()[0]\n",
    "\n",
    "print(f\"The minimum temperature for year 2012: {min2012}\")\n",
    "print(f\"The maximum temperature for year 2016: {max2016}\")\n",
    "print(f\"The number of entries for year 2018: {entries2018}\")\n",
    "print(f\"The average wind speed for year 2008: {wind2008}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59c6c1ec-cec0-4b69-b545-5b94f92e20b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct result: minimum temperature for year 2012 19.0 °F\ncorrect result: maximum temperature for year 2016 is 99.0 °F\ncorrect result: there are 228 entries for year 2018\ncorrect result: average wind speed for year 2008 is 6.49\n"
     ]
    }
   ],
   "source": [
    "if 18.95 < min2012 < 19.05:\n",
    "    print(\"correct result: minimum temperature for year 2012 19.0 °F\")\n",
    "else:\n",
    "    print(f\"wrong result: {min2012} != 19.0\")\n",
    "\n",
    "if 98.95 < max2016 < 99.05:\n",
    "    print(\"correct result: maximum temperature for year 2016 is 99.0 °F\")\n",
    "else:\n",
    "    print(f\"wrong result: {max2016} != 99.0\")\n",
    "\n",
    "if entries2018 == 228:\n",
    "    print(\"correct result: there are 228 entries for year 2018\")\n",
    "else:\n",
    "    print(f\"wrong result: {entries2018} != 228\")\n",
    "\n",
    "if 6.485 < wind2008 < 6.495:\n",
    "    print(\"correct result: average wind speed for year 2008 is 6.49\")\n",
    "else:\n",
    "    print(f\"wrong result: {wind2008} != 6.49\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f66632c0-9b09-4b2d-859e-9341cb8969ef",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Task 8 - One additional aggregated DataFrame\n",
    "\n",
    "Find the year that has the highest number of days that had fog.\n",
    "\n",
    "Note, days that have been marked as `heavyfog` days but not as `fog` should not be counted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1de83485-e803-4a14-a07f-8df8474eea91",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The year with the highest number of days with fog: 2007\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Assuming you already have task6DF\n",
    "# Replace \"fogcolumnname\" with the actual name of the column representing fog in your DataFrame\n",
    "\n",
    "# Create a new DataFrame with a column representing the count of fog days for each year\n",
    "fogCountDF = weatherDataFrameWithYear.groupBy(\"year\").agg(\n",
    "    F.sum(F.when(F.col(\"fogheavy\") == \"fog\", 1).otherwise(0)).alias(\"fog_days_count\")\n",
    ")\n",
    "\n",
    "# Find the year with the highest number of days with fog\n",
    "yearWithMostDaysWithFog: int = fogCountDF.orderBy(F.desc(\"fog_days_count\")).select(\"year\").first()[0]\n",
    "\n",
    "# Print the result\n",
    "print(f\"The year with the highest number of days with fog: {yearWithMostDaysWithFog}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe33729f-f7a8-4c05-a0be-714f9599d72c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrong result: 2007 != 2015\n"
     ]
    }
   ],
   "source": [
    "if yearWithMostDaysWithFog == 2015:\n",
    "    print(\"correct result: year 2015 had the highest number of days with fog (32)\")\n",
    "else:\n",
    "    print(f\"wrong result: {yearWithMostDaysWithFog} != 2015\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6cf45972-a721-48a3-8141-f29e97242893",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) Exercise-2-python (1)",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
